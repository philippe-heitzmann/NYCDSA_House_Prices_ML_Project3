{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required packages \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, HuberRegressor, ElasticNet, BayesianRidge\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>2915</td>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>2916</td>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>2917</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>2918</td>\n",
       "      <td>85</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>700</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>2919</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2919 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
       "2914  2915         160       RM         21.0     1936   Pave   NaN      Reg   \n",
       "2915  2916         160       RM         21.0     1894   Pave   NaN      Reg   \n",
       "2916  2917          20       RL        160.0    20000   Pave   NaN      Reg   \n",
       "2917  2918          85       RL         62.0    10441   Pave   NaN      Reg   \n",
       "2918  2919          60       RL         74.0     9627   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
       "0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
       "2914         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "2915         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "2916         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "2917         Lvl    AllPub  ...        0    NaN  MnPrv        Shed     700   \n",
       "2918         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "\n",
       "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0         2   2008        WD         Normal   208500.0  \n",
       "1         5   2007        WD         Normal   181500.0  \n",
       "2         9   2008        WD         Normal   223500.0  \n",
       "3         2   2006        WD        Abnorml   140000.0  \n",
       "4        12   2008        WD         Normal   250000.0  \n",
       "...     ...    ...       ...            ...        ...  \n",
       "2914      6   2006        WD         Normal        NaN  \n",
       "2915      4   2006        WD        Abnorml        NaN  \n",
       "2916      9   2006        WD        Abnorml        NaN  \n",
       "2917      7   2006        WD         Normal        NaN  \n",
       "2918     11   2006        WD         Normal        NaN  \n",
       "\n",
       "[2919 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the data and setting my features and target variables in the training data \n",
    "trainh = pd.read_csv('train (1).csv', sep=',')\n",
    "testh = pd.read_csv('test (1).csv', sep=',')\n",
    "features, target = trainh.iloc[:,:-1], trainh.iloc[:,-1]\n",
    "combineddf = pd.concat([trainh, testh], axis = 0)\n",
    "combineddf.index = list(range(0,2919))\n",
    "combineddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "str5 = 'RandomForestRegressor 60 0.2 -0.2'\n",
    "if str5[0:21] == 'RandomForestRegressor':\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key1': 1, 'key2': 2}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict5 = {'key1':1,'key2':2}\n",
    "dict((sorted(dict5.items(), key = lambda x: x[0][1], reverse = False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 346117935866.3144, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 466476705122.1967, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 111484593371.0434, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 537023554792.6425, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 538427512581.43536, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 581561099814.3546, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 594687192999.9218, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9279899867.19629, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 754849163840.9408, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 759467761711.9958, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 762056743420.2554, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 777688497719.6262, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 781618704378.2827, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 790334748772.8557, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 801557893194.3998, tolerance: 696659484.3571944\n",
      "  positive)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Ridge 128', (0.8928, 0.8742, 'Ridge', 128, 0.0)),\n",
       " ('Ridge 65', (0.8446, 0.8719, 'Ridge', 65, 0.15)),\n",
       " ('Ridge 87', (0.8652, 0.8704, 'Ridge', 87, 0.05)),\n",
       " ('Ridge 78', (0.8587, 0.8694, 'Ridge', 78, 0.1)),\n",
       " ('Ridge 60', (0.8432, 0.8621, 'Ridge', 60, 0.2)),\n",
       " ('Ridge 45', (0.8288, 0.8576, 'Ridge', 45, 0.3)),\n",
       " ('Ridge 49', (0.8324, 0.8573, 'Ridge', 49, 0.25)),\n",
       " ('Ridge 33', (0.8128, 0.8403, 'Ridge', 33, 0.35)),\n",
       " ('Ridge 30', (0.8076, 0.8359, 'Ridge', 30, 0.4)),\n",
       " ('Ridge 28', (0.7993, 0.8298, 'Ridge', 28, 0.45)),\n",
       " ('Ridge 23', (0.7857, 0.8062, 'Ridge', 23, 0.5)),\n",
       " ('Ridge 20', (0.7832, 0.8056, 'Ridge', 20, 0.55)),\n",
       " ('Ridge 19', (0.7828, 0.8052, 'Ridge', 19, 0.6)),\n",
       " ('Ridge 15', (0.7576, 0.7843, 'Ridge', 15, 0.7)),\n",
       " ('RandomForestRegressor 60',\n",
       "  (0.9789, 0.8949, 'RandomForestRegressor', 60, 0.2)),\n",
       " ('RandomForestRegressor 49',\n",
       "  (0.9785, 0.8944, 'RandomForestRegressor', 49, 0.25)),\n",
       " ('RandomForestRegressor 20',\n",
       "  (0.9752, 0.8928, 'RandomForestRegressor', 20, 0.55)),\n",
       " ('RandomForestRegressor 23',\n",
       "  (0.9753, 0.8923, 'RandomForestRegressor', 23, 0.5)),\n",
       " ('RandomForestRegressor 45',\n",
       "  (0.9781, 0.8922, 'RandomForestRegressor', 45, 0.3)),\n",
       " ('RandomForestRegressor 28',\n",
       "  (0.974, 0.8907, 'RandomForestRegressor', 28, 0.45)),\n",
       " ('RandomForestRegressor 78',\n",
       "  (0.9787, 0.8905, 'RandomForestRegressor', 78, 0.1)),\n",
       " ('RandomForestRegressor 33',\n",
       "  (0.9769, 0.89, 'RandomForestRegressor', 33, 0.35)),\n",
       " ('RandomForestRegressor 30',\n",
       "  (0.9761, 0.89, 'RandomForestRegressor', 30, 0.4)),\n",
       " ('RandomForestRegressor 65',\n",
       "  (0.9796, 0.8899, 'RandomForestRegressor', 65, 0.15)),\n",
       " ('RandomForestRegressor 19',\n",
       "  (0.9738, 0.889, 'RandomForestRegressor', 19, 0.6)),\n",
       " ('RandomForestRegressor 128',\n",
       "  (0.9778, 0.8879, 'RandomForestRegressor', 128, 0.0)),\n",
       " ('RandomForestRegressor 87',\n",
       "  (0.9794, 0.8878, 'RandomForestRegressor', 87, 0.05)),\n",
       " ('RandomForestRegressor 15',\n",
       "  (0.9696, 0.8613, 'RandomForestRegressor', 15, 0.7)),\n",
       " ('Lasso 65', (0.8447, 0.8731, 'Lasso', 65, 0.15)),\n",
       " ('Lasso 87', (0.8655, 0.871, 'Lasso', 87, 0.05)),\n",
       " ('Lasso 78', (0.8589, 0.871, 'Lasso', 78, 0.1)),\n",
       " ('Lasso 128', (0.9012, 0.8634, 'Lasso', 128, 0.0)),\n",
       " ('Lasso 60', (0.8433, 0.8614, 'Lasso', 60, 0.2)),\n",
       " ('Lasso 45', (0.8288, 0.8573, 'Lasso', 45, 0.3)),\n",
       " ('Lasso 49', (0.8325, 0.8569, 'Lasso', 49, 0.25)),\n",
       " ('Lasso 33', (0.8128, 0.8402, 'Lasso', 33, 0.35)),\n",
       " ('Lasso 30', (0.8077, 0.8361, 'Lasso', 30, 0.4)),\n",
       " ('Lasso 28', (0.7993, 0.8301, 'Lasso', 28, 0.45)),\n",
       " ('Lasso 23', (0.7857, 0.8062, 'Lasso', 23, 0.5)),\n",
       " ('Lasso 20', (0.7832, 0.8055, 'Lasso', 20, 0.55)),\n",
       " ('Lasso 19', (0.7828, 0.8052, 'Lasso', 19, 0.6)),\n",
       " ('Lasso 15', (0.7576, 0.7843, 'Lasso', 15, 0.7)),\n",
       " ('HuberRegressor 33', (0.7894, 0.8308, 'HuberRegressor', 33, 0.35)),\n",
       " ('HuberRegressor 45', (0.7698, 0.8176, 'HuberRegressor', 45, 0.3)),\n",
       " ('HuberRegressor 30', (0.7821, 0.8151, 'HuberRegressor', 30, 0.4)),\n",
       " ('HuberRegressor 28', (0.7716, 0.8093, 'HuberRegressor', 28, 0.45)),\n",
       " ('HuberRegressor 20', (0.773, 0.8032, 'HuberRegressor', 20, 0.55)),\n",
       " ('HuberRegressor 23', (0.7711, 0.8019, 'HuberRegressor', 23, 0.5)),\n",
       " ('HuberRegressor 19', (0.7718, 0.8001, 'HuberRegressor', 19, 0.6)),\n",
       " ('HuberRegressor 15', (0.7513, 0.7708, 'HuberRegressor', 15, 0.7)),\n",
       " ('HuberRegressor 49', (0.6613, 0.7706, 'HuberRegressor', 49, 0.25)),\n",
       " ('HuberRegressor 65', (0.6687, 0.7704, 'HuberRegressor', 65, 0.15)),\n",
       " ('HuberRegressor 78', (0.6679, 0.7699, 'HuberRegressor', 78, 0.1)),\n",
       " ('HuberRegressor 87', (0.6662, 0.7697, 'HuberRegressor', 87, 0.05)),\n",
       " ('HuberRegressor 128', (0.6663, 0.7682, 'HuberRegressor', 128, 0.0)),\n",
       " ('HuberRegressor 60', (0.6619, 0.7661, 'HuberRegressor', 60, 0.2)),\n",
       " ('GradientBoostingRegressor 23',\n",
       "  (0.9843, 0.9042, 'GradientBoostingRegressor', 23, 0.5)),\n",
       " ('GradientBoostingRegressor 87',\n",
       "  (0.9926, 0.896, 'GradientBoostingRegressor', 87, 0.05)),\n",
       " ('GradientBoostingRegressor 30',\n",
       "  (0.9867, 0.8959, 'GradientBoostingRegressor', 30, 0.4)),\n",
       " ('GradientBoostingRegressor 28',\n",
       "  (0.9858, 0.8937, 'GradientBoostingRegressor', 28, 0.45)),\n",
       " ('GradientBoostingRegressor 19',\n",
       "  (0.9808, 0.8923, 'GradientBoostingRegressor', 19, 0.6)),\n",
       " ('GradientBoostingRegressor 60',\n",
       "  (0.9923, 0.892, 'GradientBoostingRegressor', 60, 0.2)),\n",
       " ('GradientBoostingRegressor 20',\n",
       "  (0.9809, 0.8914, 'GradientBoostingRegressor', 20, 0.55)),\n",
       " ('GradientBoostingRegressor 45',\n",
       "  (0.9909, 0.8911, 'GradientBoostingRegressor', 45, 0.3)),\n",
       " ('GradientBoostingRegressor 33',\n",
       "  (0.9886, 0.8886, 'GradientBoostingRegressor', 33, 0.35)),\n",
       " ('GradientBoostingRegressor 78',\n",
       "  (0.9926, 0.8878, 'GradientBoostingRegressor', 78, 0.1)),\n",
       " ('GradientBoostingRegressor 65',\n",
       "  (0.992, 0.8874, 'GradientBoostingRegressor', 65, 0.15)),\n",
       " ('GradientBoostingRegressor 128',\n",
       "  (0.9925, 0.8866, 'GradientBoostingRegressor', 128, 0.0)),\n",
       " ('GradientBoostingRegressor 49',\n",
       "  (0.9914, 0.8849, 'GradientBoostingRegressor', 49, 0.25)),\n",
       " ('GradientBoostingRegressor 15',\n",
       "  (0.9574, 0.8693, 'GradientBoostingRegressor', 15, 0.7)),\n",
       " ('ElasticNet 87', (0.8034, 0.8308, 'ElasticNet', 87, 0.05)),\n",
       " ('ElasticNet 128', (0.8055, 0.8301, 'ElasticNet', 128, 0.0)),\n",
       " ('ElasticNet 78', (0.8026, 0.8298, 'ElasticNet', 78, 0.1)),\n",
       " ('ElasticNet 65', (0.7975, 0.8298, 'ElasticNet', 65, 0.15)),\n",
       " ('ElasticNet 60', (0.7962, 0.8266, 'ElasticNet', 60, 0.2)),\n",
       " ('ElasticNet 45', (0.7889, 0.824, 'ElasticNet', 45, 0.3)),\n",
       " ('ElasticNet 49', (0.7921, 0.8233, 'ElasticNet', 49, 0.25)),\n",
       " ('ElasticNet 33', (0.7839, 0.8198, 'ElasticNet', 33, 0.35)),\n",
       " ('ElasticNet 30', (0.7774, 0.8139, 'ElasticNet', 30, 0.4)),\n",
       " ('ElasticNet 28', (0.7742, 0.8115, 'ElasticNet', 28, 0.45)),\n",
       " ('ElasticNet 19', (0.7628, 0.7985, 'ElasticNet', 19, 0.6)),\n",
       " ('ElasticNet 20', (0.7627, 0.7983, 'ElasticNet', 20, 0.55)),\n",
       " ('ElasticNet 23', (0.7664, 0.7976, 'ElasticNet', 23, 0.5)),\n",
       " ('ElasticNet 15', (0.7303, 0.7563, 'ElasticNet', 15, 0.7)),\n",
       " ('BayesianRidge 128', (0.8819, 0.87, 'BayesianRidge', 128, 0.0)),\n",
       " ('BayesianRidge 87', (0.8606, 0.8659, 'BayesianRidge', 87, 0.05)),\n",
       " ('BayesianRidge 65', (0.8408, 0.864, 'BayesianRidge', 65, 0.15)),\n",
       " ('BayesianRidge 78', (0.855, 0.8628, 'BayesianRidge', 78, 0.1)),\n",
       " ('BayesianRidge 60', (0.8405, 0.8602, 'BayesianRidge', 60, 0.2)),\n",
       " ('BayesianRidge 45', (0.827, 0.8549, 'BayesianRidge', 45, 0.3)),\n",
       " ('BayesianRidge 49', (0.8305, 0.8543, 'BayesianRidge', 49, 0.25)),\n",
       " ('BayesianRidge 33', (0.8116, 0.8382, 'BayesianRidge', 33, 0.35)),\n",
       " ('BayesianRidge 30', (0.8066, 0.8339, 'BayesianRidge', 30, 0.4)),\n",
       " ('BayesianRidge 28', (0.7977, 0.8266, 'BayesianRidge', 28, 0.45)),\n",
       " ('BayesianRidge 23', (0.7853, 0.8068, 'BayesianRidge', 23, 0.5)),\n",
       " ('BayesianRidge 20', (0.7829, 0.8062, 'BayesianRidge', 20, 0.55)),\n",
       " ('BayesianRidge 19', (0.7825, 0.8058, 'BayesianRidge', 19, 0.6)),\n",
       " ('BayesianRidge 15', (0.7575, 0.7838, 'BayesianRidge', 15, 0.7)),\n",
       " ('AdaBoostRegressor 45', (0.8718, 0.846, 'AdaBoostRegressor', 45, 0.3)),\n",
       " ('AdaBoostRegressor 65', (0.8766, 0.8458, 'AdaBoostRegressor', 65, 0.15)),\n",
       " ('AdaBoostRegressor 49', (0.8758, 0.843, 'AdaBoostRegressor', 49, 0.25)),\n",
       " ('AdaBoostRegressor 128', (0.8695, 0.8421, 'AdaBoostRegressor', 128, 0.0)),\n",
       " ('AdaBoostRegressor 33', (0.8713, 0.84, 'AdaBoostRegressor', 33, 0.35)),\n",
       " ('AdaBoostRegressor 30', (0.8605, 0.8394, 'AdaBoostRegressor', 30, 0.4)),\n",
       " ('AdaBoostRegressor 87', (0.8692, 0.838, 'AdaBoostRegressor', 87, 0.05)),\n",
       " ('AdaBoostRegressor 20', (0.8445, 0.8345, 'AdaBoostRegressor', 20, 0.55)),\n",
       " ('AdaBoostRegressor 60', (0.8734, 0.8318, 'AdaBoostRegressor', 60, 0.2)),\n",
       " ('AdaBoostRegressor 78', (0.8704, 0.8309, 'AdaBoostRegressor', 78, 0.1)),\n",
       " ('AdaBoostRegressor 15', (0.8178, 0.8291, 'AdaBoostRegressor', 15, 0.7)),\n",
       " ('AdaBoostRegressor 19', (0.8448, 0.827, 'AdaBoostRegressor', 19, 0.6)),\n",
       " ('AdaBoostRegressor 23', (0.8583, 0.8255, 'AdaBoostRegressor', 23, 0.5)),\n",
       " ('AdaBoostRegressor 28', (0.8584, 0.8232, 'AdaBoostRegressor', 28, 0.45))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Feature_Prep(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.featuredict = {}\n",
    "        self.objectlist = []\n",
    "        self.numlist = []\n",
    "        self.df1 = pd.DataFrame(np.repeat(0,1460))\n",
    "        self.trainingdf = None\n",
    "        self.testingdf = None\n",
    "        self.dict1 = {}\n",
    "        self.trainingsubset = None\n",
    "        self.testingsubset = None\n",
    "        self.model = None \n",
    "        self.x_train = None          \n",
    "        self.x_test = None           \n",
    "        self.y_train = None          #y_train of target\n",
    "        self.y_test = None           #y_test of target\n",
    "        self.target = None\n",
    "        self.models = ['Ridge', 'Lasso', 'ElasticNet', 'BayesianRidge', 'HuberRegressor', 'AdaBoostRegressor', 'GradientBoostingRegressor', 'RandomForestRegressor']\n",
    "        self.results_dict1 = {}\n",
    "        self.results_dict2 = {}\n",
    "        self.feature_name_list = []\n",
    "        self.dict_ridge = {}\n",
    "        self.dict_lasso = {}\n",
    "        self.dict_elasticnet = {}\n",
    "        self.dict_bayesianridge = {}\n",
    "        self.dict_huberregressor = {}\n",
    "        self.dict_adaboost = {}\n",
    "        self.dict_gradientboosting = {}\n",
    "        self.dict_randomforest = {}\n",
    "        self.masterlist = []\n",
    "        \n",
    "    def type_filter(self, X):    #X is your trainh dataset\n",
    "        for idx, x in enumerate(X.dtypes):\n",
    "            self.featuredict[X.columns[idx]] = x \n",
    "        for i in list(filter(lambda x: x[1] == 'O', self.featuredict.items())):\n",
    "            self.objectlist.append(i[0])\n",
    "        return self.objectlist\n",
    "            \n",
    "    def dummify(self, X, model, posinterval, negcutoff):       #X is your combineddf dataset\n",
    "        self.target = X.iloc[0:1460, -1]\n",
    "        self.type_filter(X)\n",
    "        for feature in self.objectlist:\n",
    "            self.df1 = pd.concat([self.df1, pd.get_dummies(X[feature], prefix = feature, drop_first=True)], axis = 1)\n",
    "        for i in list(filter(lambda x: x[1] == 'int64' or x[1] == 'float64', self.featuredict.items())):\n",
    "            try:\n",
    "                self.df1 = pd.concat([self.df1, X[i[0]]], axis = 1)\n",
    "            except:\n",
    "                continue\n",
    "        self.trainingdf = self.df1.iloc[0:1460,1:]\n",
    "        self.testingdf = self.df1.iloc[1460:,1:-1]\n",
    "        self.trainingdf = self.trainingdf.fillna(self.trainingdf.mean())\n",
    "        self.testingdf = self.testingdf.fillna(self.testingdf.mean())\n",
    "        for model in self.models:\n",
    "            self.set_model(model)\n",
    "            for cutoff in posinterval:\n",
    "                dict1 = {}\n",
    "                for idx, i in enumerate(self.trainingdf.corr().iloc[:,-1]):\n",
    "                    if i > cutoff or i < negcutoff:\n",
    "                        dict1[self.trainingdf.columns[idx]] = i\n",
    "                    else:\n",
    "                        continue\n",
    "                self.feature_name_list = [x[0] for x in dict1.items() if x[0] != 'SalePrice']\n",
    "                self.trainingsubset = self.trainingdf[self.feature_name_list]\n",
    "                self.testingsubset = self.testingdf[self.feature_name_list]\n",
    "\n",
    "                self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.trainingsubset, self.target, test_size = 0.2, random_state = 42)\n",
    "\n",
    "                self.model.fit(self.x_train, self.y_train)\n",
    "                key1 = model + ' ' + str(len(self.feature_name_list))\n",
    "                self.results_dict1[key1] = (np.round(self.model.score(self.x_train, self.y_train),4), np.round(self.model.score(self.x_test, self.y_test),4), model, len(self.feature_name_list), np.round(cutoff,2))\n",
    "        \n",
    "        print(len(self.results_dict1))\n",
    "        return sorted(self.results_dict1.items(), key = lambda x: (x[1][2],x[1][1]), reverse = True)\n",
    "#         for k, v in self.results_dict1.items():\n",
    "#             if k[0:5] == 'Ridge':\n",
    "#                 try:\n",
    "#                     self.dict_ridge[k] = v \n",
    "#                 except:\n",
    "#                     continue\n",
    "#             if k[0:5] == 'Lasso':\n",
    "#                 try:\n",
    "#                     self.dict_lasso[k] = v  \n",
    "#                 except:\n",
    "#                     continue\n",
    "#             if k[0:10] == 'ElasticNet':\n",
    "#                 try:\n",
    "#                     self.dict_elasticnet[k] = v\n",
    "#                 except:\n",
    "#                     continue\n",
    "#             if k[0:14] == 'HuberRegressor':\n",
    "#                 try:\n",
    "#                     self.dict_huberregressor[k] = v\n",
    "#                 except:\n",
    "#                     continue\n",
    "#             if k[0:13] == 'BayesianRidge':\n",
    "#                 try:\n",
    "#                     self.dict_bayesianridge[k] = v\n",
    "#                 except:\n",
    "#                     continue\n",
    "#             if k[0:17] == 'AdaBoostRegressor':\n",
    "#                 try:\n",
    "#                     self.dict_adaboost[k] = v\n",
    "#                 except:\n",
    "#                     continue\n",
    "#             if k[0:25] == 'GradientBoostingRegressor':\n",
    "#                 try:\n",
    "#                     self.dict_gradientboosting[k] = v\n",
    "#                 except:\n",
    "#                     continue\n",
    "#             if k[0:21] == 'RandomForestRegressor':\n",
    "#                 try:\n",
    "#                     self.dict_randomforest[k] == v\n",
    "#                 except:\n",
    "#                     continue\n",
    "                \n",
    "#         self.dict_ridge = dict(sorted(self.dict_ridge.items(), key = lambda x: x[1][1], reverse = True))\n",
    "#         self.dict_lasso = dict(sorted(self.dict_lasso.items(), key = lambda x: x[1][1], reverse = True))\n",
    "#         self.dict_elasticnet = dict(sorted(self.dict_elasticnet.items(), key = lambda x: x[1][1], reverse = True))\n",
    "#         self.dict_huberregressor = dict(sorted(self.dict_huberregressor.items(), key = lambda x: x[1][1], reverse = True))\n",
    "#         self.dict_bayesianridge = dict(sorted(self.dict_bayesianridge.items(), key = lambda x: x[1][1], reverse = True))\n",
    "#         self.dict_adaboost = dict(sorted(self.dict_adaboost.items(), key = lambda x: x[1][1], reverse = True))\n",
    "#         self.dict_gradientboosting = dict(sorted(self.dict_gradientboosting.items(), key = lambda x: x[1][1], reverse = True))\n",
    "#         self.dict_randomforest = dict(sorted(self.dict_randomforest.items(), key = lambda x: x[1][1], reverse = True))\n",
    "        \n",
    "    def best_features(self, X, model, posinterval, negcutoff):\n",
    "        for i in range(0,4):\n",
    "            self.dummify(X, model, posinterval, negcutoff)\n",
    "            self.masterlist.append(dict(list(self.dict_ridge.items())[0:1]))\n",
    "            self.masterlist.append(dict(list(self.dict_lasso.items())[0:1]))\n",
    "            self.masterlist.append(dict(list(self.dict_elasticnet.items())[0:1]))\n",
    "            self.masterlist.append(dict(list(self.dict_huberregressor.items())[0:1]))\n",
    "            self.masterlist.append(dict(list(self.dict_bayesianridge.items())[0:1]))\n",
    "            self.masterlist.append(dict(list(self.dict_adaboost.items())[0:1]))\n",
    "            self.masterlist.append(dict(list(self.dict_gradientboosting.items())[0:1]))\n",
    "            self.masterlist.append(dict(list(self.dict_randomforest.items())[0:1]))\n",
    "        print(self.masterlist)\n",
    "        pass\n",
    "\n",
    "#                 print(sorted(self.dict_ridge.items(), key = lambda x: x[1][1], reverse = True))\n",
    "#                 print(sorted(self.dict_lasso.items(), key = lambda x: x[1][1], reverse = True))\n",
    "#                 print(sorted(self.dict_elasticnet.items(), key = lambda x: x[1][1], reverse = True))\n",
    "#                 print(sorted(self.dict_huberregressor.items(), key = lambda x: x[1][1], reverse = True))\n",
    "#                 print(sorted(self.dict_bayesianridge.items(), key = lambda x: x[1][1], reverse = True))\n",
    "#                 print(sorted(self.dict_adaboost.items(), key = lambda x: x[1][1], reverse = True))\n",
    "#                 print(sorted(self.dict_gradientboosting.items(), key = lambda x: x[1][1], reverse = True))\n",
    "#                 print(sorted(self.dict_randomforest.items(), key = lambda x: x[1][1], reverse = True))\n",
    "\n",
    "#                 [('GradientBoostingRegressor 23 0.5 -0.2', (0.9843, 0.9029)), ('GradientBoostingRegressor 65 0.15 -0.2', (0.992, 0.9006)), ('GradientBoostingRegressor 78 0.1 -0.2', (0.9926, 0.8972)), ('GradientBoostingRegressor 28 0.45 -0.2', (0.9858, 0.8956)), ('GradientBoostingRegressor 30 0.4 -0.2', (0.9867, 0.8954)), ('GradientBoostingRegressor 128 0.0 -0.2', (0.9925, 0.8928)), ('GradientBoostingRegressor 87 0.05 -0.2', (0.9926, 0.8927)), ('GradientBoostingRegressor 19 0.6 -0.2', (0.9808, 0.8924)), ('GradientBoostingRegressor 49 0.25 -0.2', (0.9914, 0.8916)), ('GradientBoostingRegressor 20 0.55 -0.2', (0.9809, 0.8916)), ('GradientBoostingRegressor 60 0.2 -0.2', (0.9923, 0.8911)), ('GradientBoostingRegressor 45 0.3 -0.2', (0.9909, 0.8861)), ('GradientBoostingRegressor 33 0.35 -0.2', (0.9886, 0.8817)), ('GradientBoostingRegressor 15 0.7 -0.2', (0.9574, 0.8696)), ('GradientBoostingRegressor 15 0.65 -0.2', (0.9574, 0.8691))]\n",
    "        \n",
    "#         self.results_dict2[list(filter(lambda x: x[0][0:5] == 'Ridge', self.results_dict1.items()))[0] = sorted(list(filter(lambda x: x[0][0:5] == 'Ridge', self.results_dict1.items())))\n",
    "#         self.results_dict2[list(filter(lambda x: x[0][0:5] == 'Lasso', self.results_dict1.items()))[:1][0][0]] = list(filter(lambda x: x[0][0:5] == 'Lasso', self.results_dict1.items()))[:1][0][1]\n",
    "#         self.results_dict2[list(filter(lambda x: x[0][0:10] == 'ElasticNet', self.results_dict1.items()))[:1][0][0]] = list(filter(lambda x: x[0][0:10] == 'ElasticNet', self.results_dict1.items()))[:1][0][1]\n",
    "#         self.results_dict2[list(filter(lambda x: x[0][0:14] == 'HuberRegressor', self.results_dict1.items()))[:1][0][0]] = list(filter(lambda x: x[0][0:14] == 'HuberRegressor', self.results_dict1.items()))[:1][0][1]\n",
    "#         self.results_dict2[list(filter(lambda x: x[0][0:13] == 'BayesianRidge', self.results_dict1.items()))[:1][0][0]] = list(filter(lambda x: x[0][0:13] == 'BayesianRidge', self.results_dict1.items()))[:1][0][1]\n",
    "#         self.results_dict2[list(filter(lambda x: x[0][0:17] == 'AdaBoostRegressor', self.results_dict1.items()))[:1][0][0]] = list(filter(lambda x: x[0][0:17] == 'AdaBoostRegressor', self.results_dict1.items()))[:1][0][1]\n",
    "#         self.results_dict2[list(filter(lambda x: x[0][0:25] == 'GradientBoostingRegressor', self.results_dict1.items()))[:1][0][0]] = list(filter(lambda x: x[0][0:25] == 'GradientBoostingRegressor', self.results_dict1.items()))[:1][0][1]\n",
    "#         self.results_dict2[list(filter(lambda x: x[0][0:21] == 'RandomForestRegressor', self.results_dict1.items()))[:1][0][0]] = list(filter(lambda x: x[0][0:21] == 'RandomForestRegressor', self.results_dict1.items()))[:1][0][1]\n",
    "#         print(self.results_dict2)\n",
    "        \n",
    "#         print(list(filter(lambda x: x[0][0:5] == 'Ridge', self.results_dict1.items())))\n",
    "                \n",
    "    \n",
    "    def set_model(self, *args):\n",
    "        if 'Ridge' in args:\n",
    "            self.model = Ridge(max_iter = 500)\n",
    "        elif 'Lasso' in args: \n",
    "            self.model = Lasso(max_iter = 500) \n",
    "        elif 'ElasticNet' in args:\n",
    "            self.model = ElasticNet(max_iter = 500)\n",
    "        elif 'HuberRegressor' in args:\n",
    "            self.model = HuberRegressor(max_iter = 500)\n",
    "        elif 'BayesianRidge' in args:\n",
    "            self.model = BayesianRidge(n_iter = 500)\n",
    "        elif 'AdaBoostRegressor' in args:\n",
    "            self.model = AdaBoostRegressor()\n",
    "        elif 'GradientBoostingRegressor' in args:\n",
    "            self.model = GradientBoostingRegressor(n_estimators = 500)\n",
    "        elif 'RandomForestRegressor' in args:\n",
    "            self.model = RandomForestRegressor(n_estimators = 500)\n",
    "        else:\n",
    "            print(\"Please choose one of the supported sklearn Regressor objects\")\n",
    "       \n",
    "\n",
    "f1 = Feature_Prep()\n",
    "f1.dummify(combineddf, 'GradientBoostingRegressor', np.linspace(0,0.7,15), -0.2)\n",
    "# f1.best_features(combineddf, 'GradientBoostingRegressor', np.linspace(0,0.7,15), -0.2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RandomForestRegressor'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1 = 'RandomForestRegressor 128 0.0 -0.2'\n",
    "str1[0:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.set_model('Lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dummify() missing 3 required positional arguments: 'model', 'posinterval', and 'negcutoff'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-a96893986c4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureSelection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombineddf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;31m# x1.find_features(combineddf, 0.5, -0.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-a96893986c4c>\u001b[0m in \u001b[0;36mfitcheck\u001b[0;34m(self, X, Y, size)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposcorrinterval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposcutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegcutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-a96893986c4c>\u001b[0m in \u001b[0;36mfind_features\u001b[0;34m(self, X, poscutoff, negcutoff)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposcutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m#X is your combineddf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposcutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposcutoff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegcutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegcutoff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: dummify() missing 3 required positional arguments: 'model', 'posinterval', and 'negcutoff'"
     ]
    }
   ],
   "source": [
    "#child class of Feature_Prep\n",
    "#fitcheck method allows to select feature subsets and fits these with different models provided in self.models \n",
    "class FeatureSelection(Feature_Prep): \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.poscutoff = 0.5\n",
    "        self.negcutoff = -0.5\n",
    "        self.featurenames = None\n",
    "        self.model = Ridge()\n",
    "        self.models = ['Ridge', 'Lasso', 'ElasticNet', 'BayesianRidge', 'DecisionTreeRegressor', 'AdaBoostRegressor', 'GradientBoostingRegressor', 'RandomForestRegressor']\n",
    "        self.trainsubset = None      #trainhsubset                     #is there a better way to initialize these self attributes?\n",
    "        self.testsubset = None       #testhsubset\n",
    "        self.testpredictions = None\n",
    "        self.x_train = None          #x_train of trainhsubset\n",
    "        self.x_test = None           #x_test of trainhsubset\n",
    "        self.y_train = None          #y_train of target\n",
    "        self.y_test = None           #y_test of target\n",
    "        self.localtime = time.asctime( time.localtime(time.time()) )\n",
    "        self.resultsdict = {}\n",
    "        self.resultsdict1 = {}\n",
    "        self.poscorrinterval = list(np.linspace(0,.75,10))\n",
    "    \n",
    "  \n",
    "    def find_features(self, X, poscutoff, negcutoff):    #X is your combineddf\n",
    "        self.dummify(X)\n",
    "        self.poscutoff = poscutoff\n",
    "        self.negcutoff = negcutoff\n",
    "        X0 = self.trainingdf.corr().iloc[:,-1]\n",
    "        dict1 = {}\n",
    "        for idx, i in enumerate(X0): \n",
    "            if i > poscutoff:\n",
    "                dict1[X0.index[idx]] = i\n",
    "            elif i < negcutoff:\n",
    "                dict1[X0.index[idx]] = i\n",
    "            else: \n",
    "                continue\n",
    "        self.featurenames = [x[0] for x in list(filter(lambda x: x[0] != 'SalePrice', dict1.items()))]\n",
    "        self.trainsubset, self.testsubet = self.trainingdf[self.featurenames], self.testingdf[self.featurenames]        \n",
    "        \n",
    "    def train_test_split(self, Y, size):   #Y is your target y variable pd Series \n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.trainsubset, Y, test_size = size)\n",
    "#         print('x_train NAs are',self.x_train.isna().sum()[200:], 'y_train NAs are', self.y_train.isna().sum())\n",
    "    \n",
    "    def fitcheck(self, X, Y, size):       #X is your combineddf, Y is your target y variable pd Series\n",
    "    \n",
    "        for model in self.models:\n",
    "            \n",
    "            self.set_model(model)\n",
    "            \n",
    "            for i in self.poscorrinterval:\n",
    "                self.find_features(X, poscutoff = i, negcutoff = -0.5)\n",
    "                self.train_test_split(Y, size) \n",
    "                self.model.fit(self.x_train, self.y_train)\n",
    "                key1 = model + ' ' + str(len(self.featurenames)) + ' ' + str(i)\n",
    "                self.resultsdict1[key1] = [np.round(self.model.score(self.x_train, self.y_train),3), np.round(self.model.score(self.x_test, self.y_test),3)]\n",
    "                \n",
    "        return sorted(self.resultsdict1.items(), key = lambda x: x[1][1], reverse = True)\n",
    "\n",
    "    \n",
    "    def set_model(self, *args):\n",
    "        if 'Ridge' in args:\n",
    "            self.model = Ridge()\n",
    "            return self.model\n",
    "        elif 'Lasso' in args: \n",
    "            self.model = Lasso() \n",
    "            return self.model\n",
    "        elif 'ElasticNet' in args:\n",
    "            self.model = ElasticNet()\n",
    "            return self.model\n",
    "        elif 'HuberRegressor' in args:\n",
    "            self.model = HuberRegressor()\n",
    "            return self.model\n",
    "        elif 'BayesianRidge' in args:\n",
    "            self.model = BayesianRidge()\n",
    "            return self.model\n",
    "        elif 'DecisionTreeRegressor' in args:   #may make sense to not include \n",
    "            self.model = DecisionTreeRegressor()\n",
    "            return self.model\n",
    "        elif 'AdaBoostRegressor' in args:\n",
    "            self.model = AdaBoostRegressor()\n",
    "            return self.model\n",
    "        elif 'GradientBoostingRegressor' in args:\n",
    "            self.model = GradientBoostingRegressor()\n",
    "            return self.model\n",
    "        elif 'RandomForestRegressor' in args:\n",
    "            self.model = RandomForestRegressor()\n",
    "            return self.model\n",
    "        else:\n",
    "            return self.model\n",
    "    \n",
    "    def fit(self, X, Y):     #Y is your target y variable pd Series \n",
    "        self.model.fit(X, Y)\n",
    "    \n",
    "    \n",
    "    def predict(self):    \n",
    "        self.testpredictions = self.model.predict(self.testsubset)\n",
    "    \n",
    "    #static method declarators - don't need access to methods Look into function scope \n",
    "#     @staticmethod\n",
    "\n",
    "    def preprocess(self, X, Y):   #Y is your testh dataset, X is your trainh dataset\n",
    "        if isinstance(X, list): raise TypeError(\"X needs to be a DataFrame object\")\n",
    "        elif isinstance(X, np.ndarray): raise TypeError()\n",
    "#         X = X.fillna(X.mean())    #Q: would this work? Why not?\n",
    "#         print(X.isna().sum(),'\\n')\n",
    "#         Y = Y.fillna(Y.mean())\n",
    "#         print(Y.isna().sum())\n",
    "        self.trainsubset = X.fillna(X.mean())\n",
    "        self.testsubset = Y.fillna(Y.mean()) \n",
    "        print(self.testsubset.isna().sum())\n",
    "#         print(self.testsubset.isna().sum())\n",
    "        \n",
    "        \n",
    "    def to_csv1(self):\n",
    "        testpredictionsdf = pd.DataFrame(self.testpredictions, index = range(1461, 2920), columns = ['SalePrice'])\n",
    "        testpredictionsdf.index.name = 'Id'\n",
    "        testpredictionsdf.to_csv('Kaggle Submission %s .csv'%(self.localtime))\n",
    "\n",
    "\n",
    "x1 = FeatureSelection() \n",
    "x1.fitcheck(combineddf, target, 0.2)\n",
    "# x1.find_features(combineddf, 0.5, -0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = FeatureSelection()\n",
    "x1.df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philippeheitzmann/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 925125760996.3031, tolerance: 920791133.4609975\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "#child class of FeatureSelection\n",
    "#sets up hyperparameter tuning for the best models \n",
    "\n",
    "class KaggleOutput(FeatureSelection):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.paramgrid = {}\n",
    "        self.df3 = None  \n",
    "        self.dict3 = {}\n",
    "        \n",
    "    def prep(self, X, model, cutoff, negcutoff):\n",
    "        self.target = X.iloc[0:1460, -1]\n",
    "        self.type_filter(X)\n",
    "        for feature in self.objectlist:\n",
    "            self.df3 = pd.concat([self.df3, pd.get_dummies(X[feature], prefix = feature, drop_first=True)], axis = 1)\n",
    "        for i in list(filter(lambda x: x[1] == 'int64' or x[1] == 'float64', self.featuredict.items())):\n",
    "            try:\n",
    "                self.df3 = pd.concat([self.df3, X[i[0]]], axis = 1)\n",
    "            except:\n",
    "                continue\n",
    "        self.trainingdf = self.df3.iloc[0:1460,1:]\n",
    "        self.testingdf = self.df3.iloc[1460:,1:-1]\n",
    "        self.trainingdf = self.trainingdf.fillna(self.trainingdf.mean())\n",
    "        self.testingdf = self.testingdf.fillna(self.testingdf.mean())\n",
    "        \n",
    "        self.set_model(model)\n",
    "        \n",
    "        for idx, i in enumerate(self.trainingdf.corr().iloc[:,-1]):   #could just make dict3 into a list \n",
    "            if i > cutoff or i < negcutoff:\n",
    "                self.dict3[self.trainingdf.columns[idx]] = i\n",
    "            else:\n",
    "                continue\n",
    "        self.feature_name_list = [x[0] for x in self.dict3.items() if x[0] != 'SalePrice']\n",
    "        self.trainingsubset = self.trainingdf[self.feature_name_list]\n",
    "        self.testingsubset = self.testingdf[self.feature_name_list]\n",
    "\n",
    "        self.model.fit(self.trainingsubset, self.target)\n",
    "        \n",
    "        predictiondf = pd.DataFrame(self.model.predict(self.trainingsubset), index = range(0, 1460), columns = [str(self.model)])\n",
    "#         predictiondf.index.name = 'Id'\n",
    "        predictiondf.to_csv('Kaggle Submission TrueTrain %s .csv'%(str(self.model) + ' ' + str(len(self.feature_name_list)) + ' ' + self.localtime))\n",
    "        \n",
    "    def kaggle_output(self, X, Y, Y2, model, paramgrid, poscutoff, negcutoff):    #Y is target, Y2 is testh\n",
    "        \n",
    "        self.prep(combineddf)\n",
    "        \n",
    "        self.set_model(model)\n",
    "        try: \n",
    "            self.model.set_params(paramgrid)\n",
    "        except:\n",
    "            pass\n",
    "        self.find_features(X, Y2, poscutoff, negcutoff)\n",
    "        self.fit(self.trainsubset, Y)\n",
    "        self.predict()\n",
    "        self.to_csv1()\n",
    "        \n",
    "    def tuning(self, model, X, Y, Y2, paramgrid, poscutoff, negcutoff, size):    #Y is target, Y2 is testh\n",
    "        self.set_model(model)\n",
    "        self.find_features(X, Y2, poscutoff, negcutoff)\n",
    "        self.train_test_split(Y, size)\n",
    "        grid_search1 = GridSearchCV(self.model, paramgrid, scoring = 'neg_root_mean_squared_error', n_jobs = -1, cv = 5)\n",
    "        grid_search1.fit(self.x_train, self.y_train)   #cross-fold validation - check for overfitting post tuning \n",
    "        self.paramgrid = grid_search1.best_params_\n",
    "        print(self.paramgrid)\n",
    "        print('The training error is %.4f' %(1 - grid_search1.best_estimator_.score(self.x_train, self.y_train)))\n",
    "        print('The testing error is %.4f' %(1 - grid_search1.best_estimator_.score(self.x_test, self.y_test)))\n",
    "        self.kaggle_output(X, Y, Y2, model, paramgrid, poscutoff, negcutoff)\n",
    "        \n",
    "        \n",
    "x2 = KaggleOutput()\n",
    "# x2.kaggle_output(trainh, target, testh, 'GradientBoostingRegressor', 0.23275862068965517, -0.5)\n",
    "# x2.kaggle_output(trainh, target, testh, 'RandomForestRegressor', 0.33620689655172414, -0.5)\n",
    "# x2.kaggle_output(trainh, target, testh, 'RandomForestRegressor', 0.1810344827586207, -0.5)\n",
    "# x2.tuning('GradientBoostingRegressor', trainh, target, testh, {'learning_rate':np.linspace(0.01,0.1,10), 'subsample':np.linspace(0.3,1,5)}, 0.23275862068965517, -0.5, 0.2)\n",
    "x2.prep(combineddf, 'ElasticNet', 0.130000, -0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data predictions \n",
    "#loading the predictions of the eight different regressors for the training data\n",
    "\n",
    "ada_boostdf2 = pd.read_csv('Kaggle Submission TrueTrain AdaBoostRegressor() 41 Sat Sep 19 19:49:02 2020 .csv', sep = ',')\n",
    "ada_boostdf2 = ada_boostdf2.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "bayesian_ridgedf2 = pd.read_csv('Kaggle Submission TrueTrain BayesianRidge() 84 Sat Sep 19 19:48:45 2020 .csv', sep = ',')\n",
    "bayesian_ridgedf2 = bayesian_ridgedf2.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "elastic_netdf2 = pd.read_csv('Kaggle Submission TrueTrain ElasticNet() 68 Sat Sep 19 19:50:37 2020 .csv', sep = ',')\n",
    "elastic_netdf2 = elastic_netdf2.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "gradient_boosting_regressordf2 = pd.read_csv('Kaggle Submission TrueTrain GradientBoostingRegressor() 30 Sat Sep 19 19:48:23 2020 .csv', sep = ',')\n",
    "gradient_boosting_regressordf2 = gradient_boosting_regressordf2.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "huber_regressordf2 = pd.read_csv('Kaggle Submission TrueTrain HuberRegressor() 33 Sat Sep 19 19:48:08 2020 .csv', sep = ',')\n",
    "huber_regressordf2 = huber_regressordf2.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "lassodf2 = pd.read_csv('Kaggle Submission TrueTrain Lasso() 78 Sat Sep 19 19:47:58 2020 .csv', sep = ',')\n",
    "lassodf2 = lassodf2.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "random_forestdf2 = pd.read_csv('Kaggle Submission TrueTrain RandomForestRegressor() 46 Sat Sep 19 19:47:49 2020 .csv', sep = ',')\n",
    "random_forestdf2 = random_forestdf2.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "ridgedf2 = pd.read_csv('Kaggle Submission TrueTrain Ridge() 84 Sat Sep 19 19:47:37 2020 .csv', sep = ',')\n",
    "ridgedf2 = ridgedf2.drop(['Unnamed: 0'], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing data\n",
    "#loading the predictions of the eight different regressors for the testing data\n",
    "\n",
    "ada_boostdf = pd.read_csv('Kaggle Submission AdaBoostRegressor() 41 Sat Sep 19 19:08:46 2020 .csv', sep = ',')\n",
    "ada_boostdf = ada_boostdf.drop(['Id'], axis = 1)\n",
    "ada_boostdf.columns = ['AdaBoost']\n",
    "\n",
    "bayesian_ridgedf = pd.read_csv('Kaggle Submission BayesianRidge() 84 Sat Sep 19 19:08:34 2020 .csv', sep = ',')\n",
    "bayesian_ridgedf = bayesian_ridgedf.drop(['Id'], axis = 1)\n",
    "bayesian_ridgedf.columns = ['BayesianRidge']\n",
    "\n",
    "elastic_netdf = pd.read_csv('Kaggle Submission ElasticNet() 68 Sat Sep 19 19:08:18 2020 .csv', sep = ',')\n",
    "elastic_netdf = elastic_netdf.drop(['Id'], axis = 1)\n",
    "elastic_netdf.columns = ['ElasticNet']\n",
    "\n",
    "gradient_boosting_regressordf = pd.read_csv('Kaggle Submission GradientBoostingRegressor() 30 Sat Sep 19 19:08:05 2020 .csv', sep = ',')\n",
    "gradient_boosting_regressordf = gradient_boosting_regressordf.drop(['Id'], axis = 1)\n",
    "gradient_boosting_regressordf.columns = ['GradientBoostingR']\n",
    "\n",
    "huber_regressordf = pd.read_csv('Kaggle Submission HuberRegressor() 33 Sat Sep 19 19:07:49 2020 .csv', sep = ',')\n",
    "huber_regressordf = huber_regressordf.drop(['Id'], axis = 1)\n",
    "huber_regressordf.columns = ['HuberRegressor']\n",
    "\n",
    "lassodf = pd.read_csv('Kaggle Submission Lasso() 78 Sat Sep 19 19:06:58 2020 .csv', sep = ',')\n",
    "lassodf = lassodf.drop(['Id'], axis = 1)\n",
    "lassodf.columns = ['Lasso']\n",
    "\n",
    "random_forestdf = pd.read_csv('Kaggle Submission RandomForestRegressor() 46 Sat Sep 19 19:06:48 2020 .csv', sep = ',')\n",
    "random_forestdf = random_forestdf.drop(['Id'], axis = 1)\n",
    "random_forestdf.columns = ['RandomForest']\n",
    "\n",
    "ridgedf = pd.read_csv('Kaggle Submission Ridge() 84 Sat Sep 19 19:06:14 2020 .csv', sep = ',')\n",
    "ridgedf = ridgedf.drop(['Id'], axis = 1)\n",
    "ridgedf.columns = ['Ridge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lm2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6e69f962e92a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lm2' is not defined"
     ]
    }
   ],
   "source": [
    "lm2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdaBoost</th>\n",
       "      <th>BayesianRidge</th>\n",
       "      <th>ElasticNet</th>\n",
       "      <th>GradientBoostingR</th>\n",
       "      <th>HuberRegressor</th>\n",
       "      <th>Lasso</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>Ridge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>209918.558219</td>\n",
       "      <td>204643.338933</td>\n",
       "      <td>216274.894771</td>\n",
       "      <td>200929.126639</td>\n",
       "      <td>205533.606866</td>\n",
       "      <td>200724.033256</td>\n",
       "      <td>205928.25</td>\n",
       "      <td>204452.286881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157568.463035</td>\n",
       "      <td>170243.370892</td>\n",
       "      <td>172925.990277</td>\n",
       "      <td>160553.412684</td>\n",
       "      <td>177892.851168</td>\n",
       "      <td>168976.299913</td>\n",
       "      <td>176221.50</td>\n",
       "      <td>170509.050471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>215471.223097</td>\n",
       "      <td>210530.840398</td>\n",
       "      <td>223279.154200</td>\n",
       "      <td>215751.855269</td>\n",
       "      <td>214515.376684</td>\n",
       "      <td>206468.171216</td>\n",
       "      <td>219537.00</td>\n",
       "      <td>209515.089784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>196621.649606</td>\n",
       "      <td>198401.512814</td>\n",
       "      <td>175824.875640</td>\n",
       "      <td>181124.727045</td>\n",
       "      <td>156037.880122</td>\n",
       "      <td>175662.106874</td>\n",
       "      <td>162009.99</td>\n",
       "      <td>199414.177471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286448.713580</td>\n",
       "      <td>305825.972255</td>\n",
       "      <td>286534.155457</td>\n",
       "      <td>290443.695034</td>\n",
       "      <td>277767.912231</td>\n",
       "      <td>313897.232469</td>\n",
       "      <td>263430.93</td>\n",
       "      <td>309958.711713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>180347.611111</td>\n",
       "      <td>168418.138700</td>\n",
       "      <td>179646.372317</td>\n",
       "      <td>174747.795139</td>\n",
       "      <td>181618.083769</td>\n",
       "      <td>170597.607264</td>\n",
       "      <td>175209.00</td>\n",
       "      <td>167732.479074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>213311.468619</td>\n",
       "      <td>189975.492778</td>\n",
       "      <td>228493.732566</td>\n",
       "      <td>205351.458569</td>\n",
       "      <td>240614.929895</td>\n",
       "      <td>186991.246856</td>\n",
       "      <td>207386.98</td>\n",
       "      <td>187724.860539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>248416.982301</td>\n",
       "      <td>254208.251693</td>\n",
       "      <td>225727.252185</td>\n",
       "      <td>263583.257529</td>\n",
       "      <td>222303.285810</td>\n",
       "      <td>238559.054166</td>\n",
       "      <td>260710.79</td>\n",
       "      <td>258415.510399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>132523.478261</td>\n",
       "      <td>157429.068970</td>\n",
       "      <td>143717.549075</td>\n",
       "      <td>137584.995036</td>\n",
       "      <td>124541.886324</td>\n",
       "      <td>161246.286058</td>\n",
       "      <td>135171.90</td>\n",
       "      <td>157394.194728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>130553.980000</td>\n",
       "      <td>160671.967762</td>\n",
       "      <td>159249.492490</td>\n",
       "      <td>135993.484577</td>\n",
       "      <td>158737.329048</td>\n",
       "      <td>155496.037415</td>\n",
       "      <td>149681.75</td>\n",
       "      <td>158992.747150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AdaBoost  BayesianRidge     ElasticNet  GradientBoostingR  \\\n",
       "0     209918.558219  204643.338933  216274.894771      200929.126639   \n",
       "1     157568.463035  170243.370892  172925.990277      160553.412684   \n",
       "2     215471.223097  210530.840398  223279.154200      215751.855269   \n",
       "3     196621.649606  198401.512814  175824.875640      181124.727045   \n",
       "4     286448.713580  305825.972255  286534.155457      290443.695034   \n",
       "...             ...            ...            ...                ...   \n",
       "1455  180347.611111  168418.138700  179646.372317      174747.795139   \n",
       "1456  213311.468619  189975.492778  228493.732566      205351.458569   \n",
       "1457  248416.982301  254208.251693  225727.252185      263583.257529   \n",
       "1458  132523.478261  157429.068970  143717.549075      137584.995036   \n",
       "1459  130553.980000  160671.967762  159249.492490      135993.484577   \n",
       "\n",
       "      HuberRegressor          Lasso  RandomForest          Ridge  \n",
       "0      205533.606866  200724.033256     205928.25  204452.286881  \n",
       "1      177892.851168  168976.299913     176221.50  170509.050471  \n",
       "2      214515.376684  206468.171216     219537.00  209515.089784  \n",
       "3      156037.880122  175662.106874     162009.99  199414.177471  \n",
       "4      277767.912231  313897.232469     263430.93  309958.711713  \n",
       "...              ...            ...           ...            ...  \n",
       "1455   181618.083769  170597.607264     175209.00  167732.479074  \n",
       "1456   240614.929895  186991.246856     207386.98  187724.860539  \n",
       "1457   222303.285810  238559.054166     260710.79  258415.510399  \n",
       "1458   124541.886324  161246.286058     135171.90  157394.194728  \n",
       "1459   158737.329048  155496.037415     149681.75  158992.747150  \n",
       "\n",
       "[1460 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenating the training data predictions \n",
    "merged_train_df = pd.concat([ada_boostdf2, bayesian_ridgedf2, elastic_netdf2, gradient_boosting_regressordf2, huber_regressordf2, lassodf2, random_forestdf2, ridgedf2], axis = 1)\n",
    "merged_train_df.columns=['AdaBoost','BayesianRidge','ElasticNet','GradientBoostingR','HuberRegressor','Lasso','RandomForest','Ridge']\n",
    "merged_train_df\n",
    "\n",
    "#splitting our data \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(merged_train_df, target, test_size = 0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9904595708265973 0.9827508181125693\n"
     ]
    }
   ],
   "source": [
    "#using simple Linear REgression to fit the train subset of the merged training predictions and seeing how these fare vs testing subset of training dataste  \n",
    "lm2 = LinearRegression()\n",
    "lm2.fit(x_train, y_train)\n",
    "print(lm2.score(x_train, y_train), lm2.score(x_test, y_test))\n",
    "merged_test_predictionsdf2 = pd.DataFrame(lm2.predict(merged_test_df), index = range(1461, 2920), columns = ['SalePrice'])\n",
    "merged_test_predictionsdf2.index.name = 'Id'\n",
    "merged_test_predictionsdf2.to_csv('Merged_test_predictions3.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "#tuning our GradientBoostingRegressor with GridSearchCV to find the best parameters on our training subset \n",
    "gbr2 = GradientBoostingRegressor()\n",
    "#'max_depth':range(2,31),\n",
    "paramgrid = {'learning_rate':np.linspace(0.5,1,5), 'n_estimators':range(100,500,100)}\n",
    "gridsearch_gbr2 = GridSearchCV(gbr2, paramgrid, cv = 5, n_jobs = -1, scoring = 'neg_root_mean_squared_error')\n",
    "gridsearch_gbr2.fit(x_train, y_train)\n",
    "print(gridsearch_gbr2.best_params_)\n",
    "\n",
    "\n",
    "# gbr2.fit(x_train, y_train)\n",
    "# print(gbr2.score(x_train, y_train), gbr2.score(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2106.1344174013225 -10411.41642229556\n"
     ]
    }
   ],
   "source": [
    "print(gridsearch_gbr2.score(x_train, y_train),gridsearch_gbr2.score(x_test, y_test))   #is 5x root MSE cause for concern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_gbr2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993187332018426 0.9753398303784183\n"
     ]
    }
   ],
   "source": [
    "#trying the best parameters to fit a new Gradient Boosting Regressor\n",
    "gbr3 = GradientBoostingRegressor(learning_rate = 0.5, n_estimators = 100)\n",
    "gbr3.fit(x_train, y_train)\n",
    "print(gbr3.score(x_train, y_train),gbr3.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the model we training on the training subsets of the stacked training predictions\n",
    "merged_test_df = pd.concat([ada_boostdf, bayesian_ridgedf, elastic_netdf, gradient_boosting_regressordf, huber_regressordf, lassodf, random_forestdf, ridgedf], axis = 1)\n",
    "merged_test_predictionsdf = pd.DataFrame(gbr3.predict(merged_test_df), index = range(1461, 2920), columns = ['SalePrice'])\n",
    "merged_test_predictionsdf.index.name = 'Id'\n",
    "merged_test_predictionsdf.to_csv('Merged_test_predictions4.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdaBoost</th>\n",
       "      <th>BayesianRidge</th>\n",
       "      <th>ElasticNet</th>\n",
       "      <th>GradientBoostingR</th>\n",
       "      <th>HuberRegressor</th>\n",
       "      <th>Lasso</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>Ridge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126932.846608</td>\n",
       "      <td>96827.588553</td>\n",
       "      <td>120181.717252</td>\n",
       "      <td>120231.346171</td>\n",
       "      <td>155100.668744</td>\n",
       "      <td>89258.417607</td>\n",
       "      <td>124882.16</td>\n",
       "      <td>93178.034492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146598.964851</td>\n",
       "      <td>150417.215895</td>\n",
       "      <td>162485.050339</td>\n",
       "      <td>148107.921691</td>\n",
       "      <td>173032.379790</td>\n",
       "      <td>150328.550023</td>\n",
       "      <td>152736.50</td>\n",
       "      <td>147853.325946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176805.524664</td>\n",
       "      <td>175330.108981</td>\n",
       "      <td>189139.983827</td>\n",
       "      <td>169918.584003</td>\n",
       "      <td>193099.998103</td>\n",
       "      <td>176456.835426</td>\n",
       "      <td>179665.00</td>\n",
       "      <td>174683.594110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183788.884058</td>\n",
       "      <td>189143.331491</td>\n",
       "      <td>202938.982581</td>\n",
       "      <td>185881.896019</td>\n",
       "      <td>187942.234484</td>\n",
       "      <td>189812.127414</td>\n",
       "      <td>187808.05</td>\n",
       "      <td>187157.692481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>228829.474954</td>\n",
       "      <td>223954.918480</td>\n",
       "      <td>192174.540561</td>\n",
       "      <td>201404.985474</td>\n",
       "      <td>173803.388146</td>\n",
       "      <td>236451.029484</td>\n",
       "      <td>205201.56</td>\n",
       "      <td>231186.243125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>118394.975078</td>\n",
       "      <td>74444.864119</td>\n",
       "      <td>76016.830582</td>\n",
       "      <td>74796.164090</td>\n",
       "      <td>84081.864902</td>\n",
       "      <td>73949.005111</td>\n",
       "      <td>82048.61</td>\n",
       "      <td>74527.736377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>118394.975078</td>\n",
       "      <td>80671.553692</td>\n",
       "      <td>89552.805055</td>\n",
       "      <td>86043.686889</td>\n",
       "      <td>116544.448907</td>\n",
       "      <td>80574.049128</td>\n",
       "      <td>84768.50</td>\n",
       "      <td>79957.929065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>155459.591304</td>\n",
       "      <td>170812.633689</td>\n",
       "      <td>176303.053845</td>\n",
       "      <td>153504.605641</td>\n",
       "      <td>186586.926183</td>\n",
       "      <td>171902.735894</td>\n",
       "      <td>153055.50</td>\n",
       "      <td>171132.100356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>136101.666667</td>\n",
       "      <td>107808.812720</td>\n",
       "      <td>118634.877237</td>\n",
       "      <td>121668.893824</td>\n",
       "      <td>101713.785216</td>\n",
       "      <td>107520.684867</td>\n",
       "      <td>109072.00</td>\n",
       "      <td>105205.493180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>252287.879004</td>\n",
       "      <td>218953.036942</td>\n",
       "      <td>235992.594139</td>\n",
       "      <td>235687.799610</td>\n",
       "      <td>238512.438145</td>\n",
       "      <td>218321.267647</td>\n",
       "      <td>218478.60</td>\n",
       "      <td>216225.192252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AdaBoost  BayesianRidge     ElasticNet  GradientBoostingR  \\\n",
       "0     126932.846608   96827.588553  120181.717252      120231.346171   \n",
       "1     146598.964851  150417.215895  162485.050339      148107.921691   \n",
       "2     176805.524664  175330.108981  189139.983827      169918.584003   \n",
       "3     183788.884058  189143.331491  202938.982581      185881.896019   \n",
       "4     228829.474954  223954.918480  192174.540561      201404.985474   \n",
       "...             ...            ...            ...                ...   \n",
       "1454  118394.975078   74444.864119   76016.830582       74796.164090   \n",
       "1455  118394.975078   80671.553692   89552.805055       86043.686889   \n",
       "1456  155459.591304  170812.633689  176303.053845      153504.605641   \n",
       "1457  136101.666667  107808.812720  118634.877237      121668.893824   \n",
       "1458  252287.879004  218953.036942  235992.594139      235687.799610   \n",
       "\n",
       "      HuberRegressor          Lasso  RandomForest          Ridge  \n",
       "0      155100.668744   89258.417607     124882.16   93178.034492  \n",
       "1      173032.379790  150328.550023     152736.50  147853.325946  \n",
       "2      193099.998103  176456.835426     179665.00  174683.594110  \n",
       "3      187942.234484  189812.127414     187808.05  187157.692481  \n",
       "4      173803.388146  236451.029484     205201.56  231186.243125  \n",
       "...              ...            ...           ...            ...  \n",
       "1454    84081.864902   73949.005111      82048.61   74527.736377  \n",
       "1455   116544.448907   80574.049128      84768.50   79957.929065  \n",
       "1456   186586.926183  171902.735894     153055.50  171132.100356  \n",
       "1457   101713.785216  107520.684867     109072.00  105205.493180  \n",
       "1458   238512.438145  218321.267647     218478.60  216225.192252  \n",
       "\n",
       "[1459 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
