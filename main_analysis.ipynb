{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# House Prices Prediction - Ames, Iowa\n",
        "\n",
        "A comprehensive machine learning project for predicting house prices using the Ames Housing dataset.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Data Loading and Initial Exploration](#data-loading)\n",
        "2. [Data Preprocessing](#data-preprocessing)\n",
        "3. [Feature Engineering](#feature-engineering)\n",
        "4. [Feature Selection](#feature-selection)\n",
        "5. [Model Training and Tuning](#model-training)\n",
        "6. [Model Stacking](#model-stacking)\n",
        "7. [Results and Analysis](#results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Initial Exploration {#data-loading}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning packages\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, HuberRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data\n",
        "train_data = pd.read_csv('train (1).csv')\n",
        "test_data = pd.read_csv('test (1).csv')\n",
        "\n",
        "# Combine datasets for preprocessing\n",
        "combined_data = pd.concat([train_data, test_data], axis=0, ignore_index=True)\n",
        "\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")\n",
        "print(f\"Combined data shape: {combined_data.shape}\")\n",
        "\n",
        "# Display basic info\n",
        "print(\"\\nFirst few rows of training data:\")\n",
        "train_data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing {#data-preprocessing}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HousePricePreprocessor:\n",
        "    \"\"\"\n",
        "    A comprehensive preprocessing class for the Ames Housing dataset.\n",
        "    \n",
        "    This class handles:\n",
        "    - Outlier detection and removal\n",
        "    - Missing value imputation\n",
        "    - Feature engineering\n",
        "    - Data transformation and encoding\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.processed_data = None\n",
        "        self.target = None\n",
        "        self.feature_names = None\n",
        "        \n",
        "    def preprocess(self, data):\n",
        "        \"\"\"\n",
        "        Main preprocessing pipeline for the housing data.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : pd.DataFrame\n",
        "            Combined training and test data\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame : Processed data ready for modeling\n",
        "        \"\"\"\n",
        "        # Create a copy to avoid modifying original data\n",
        "        df = data.copy()\n",
        "        \n",
        "        # Step 1: Handle outliers\n",
        "        df = self._handle_outliers(df)\n",
        "        \n",
        "        # Step 2: Impute missing values\n",
        "        df = self._impute_missing_values(df)\n",
        "        \n",
        "        # Step 3: Feature engineering\n",
        "        df = self._create_features(df)\n",
        "        \n",
        "        # Step 4: Encode categorical variables\n",
        "        df = self._encode_categorical_variables(df)\n",
        "        \n",
        "        # Step 5: Remove unnecessary columns\n",
        "        df = self._remove_unnecessary_columns(df)\n",
        "        \n",
        "        self.processed_data = df\n",
        "        return df\n",
        "    \n",
        "    def _handle_outliers(self, df):\n",
        "        \"\"\"Remove extreme outliers from the dataset.\"\"\"\n",
        "        # Remove GrLivArea outliers (houses with > 4500 sq ft)\n",
        "        df = df.drop(df[df['GrLivArea'] > 4500].index)\n",
        "        \n",
        "        # Remove LotFrontage outliers (lots with > 300 ft frontage)\n",
        "        df = df.drop(df[df['LotFrontage'] > 300].index)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _impute_missing_values(self, df):\n",
        "        \"\"\"Handle missing values with appropriate imputation strategies.\"\"\"\n",
        "        # MSZoning: Fill with most common value (RL)\n",
        "        df['MSZoning'] = df['MSZoning'].fillna('RL')\n",
        "        \n",
        "        # LotFrontage: Fill with neighborhood mean\n",
        "        df['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(\n",
        "            lambda x: x.fillna(x.mean())\n",
        "        ).astype(int)\n",
        "        \n",
        "        # Alley: NA means no alley access\n",
        "        df['Alley'] = df['Alley'].fillna('NoAlley')\n",
        "        df['Alley'] = df['Alley'].replace({'NoAlley': 0, 'Grvl': 1, 'Pave': 1})\n",
        "        \n",
        "        # Remove columns with too many missing values\n",
        "        df = df.drop(['Condition2', 'Utilities', 'RoofMatl', 'LowQualFinSF', 'MiscVal'], axis=1)\n",
        "        \n",
        "        # GarageYrBlt: Fill with YearBuilt\n",
        "        df['GarageYrBlt'] = df['GarageYrBlt'].fillna(df['YearBuilt'])\n",
        "        \n",
        "        # Exterior variables: Fill with mode\n",
        "        df['Exterior1st'] = df['Exterior1st'].fillna(df['Exterior1st'].mode()[0])\n",
        "        df['Exterior2nd'] = df['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0])\n",
        "        \n",
        "        # Masonry veneer: Fill with None/0\n",
        "        df['MasVnrType'] = df['MasVnrType'].fillna('None')\n",
        "        df['MasVnrArea'] = df['MasVnrArea'].fillna(0)\n",
        "        \n",
        "        # Basement variables: Fill with None/0\n",
        "        basement_vars = ['BsmtQual', 'BsmtFinType1', 'BsmtFinType2', 'BsmtCond', 'BsmtExposure']\n",
        "        for var in basement_vars:\n",
        "            df[var] = df[var].fillna('None')\n",
        "        \n",
        "        basement_area_vars = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']\n",
        "        for var in basement_area_vars:\n",
        "            df[var] = df[var].fillna(0)\n",
        "        \n",
        "        # Other missing values\n",
        "        df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])\n",
        "        df['KitchenQual'] = df['KitchenQual'].fillna(df['KitchenQual'].mode()[0])\n",
        "        df['Functional'] = df['Functional'].fillna(df['Functional'].mode()[0])\n",
        "        df['SaleType'] = df['SaleType'].fillna(df['SaleType'].mode()[0])\n",
        "        \n",
        "        # Fireplace and garage variables\n",
        "        df['FireplaceQu'] = df['FireplaceQu'].fillna('None')\n",
        "        df['GarageType'] = df['GarageType'].fillna('None')\n",
        "        df['GarageFinish'] = df['GarageFinish'].fillna('None')\n",
        "        df['GarageQual'] = df['GarageQual'].fillna('None')\n",
        "        df['GarageCond'] = df['GarageCond'].fillna('None')\n",
        "        df['PoolQC'] = df['PoolQC'].fillna('None')\n",
        "        df['Fence'] = df['Fence'].fillna('None')\n",
        "        \n",
        "        # Garage area and cars\n",
        "        df['GarageCars'] = df['GarageCars'].fillna(0)\n",
        "        df['GarageArea'] = df['GarageArea'].fillna(0)\n",
        "        \n",
        "        # Basement bathrooms\n",
        "        df['BsmtHalfBath'] = df['BsmtHalfBath'].fillna(0)\n",
        "        df['BsmtFullBath'] = df['BsmtFullBath'].fillna(0)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _create_features(self, df):\n",
        "        \"\"\"Create new engineered features.\"\"\"\n",
        "        # Age-related features\n",
        "        df['Age'] = df['YrSold'] - df['YearBuilt']\n",
        "        df['YearsSinceRemodel'] = df['YrSold'] - df['YearRemodAdd']\n",
        "        df['Remodeled'] = (df['YearBuilt'] != df['YearRemodAdd']).astype(int)\n",
        "        df['RecentRemodel'] = (df['YearRemodAdd'] == df['YrSold']).astype(int)\n",
        "        df['YearSoldYearBuilt'] = (df['YearBuilt'] == df['YrSold']).astype(int)\n",
        "        df['SqYearBuilt'] = df['YearBuilt'] ** 2\n",
        "        \n",
        "        # Area features\n",
        "        area_cols = ['LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', \n",
        "                    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n",
        "                    '3SsnPorch', 'ScreenPorch']\n",
        "        df['TotalArea'] = df[area_cols].sum(axis=1)\n",
        "        df['Area1st2nd'] = df['1stFlrSF'] + df['2ndFlrSF']\n",
        "        df['Has2ndFloor'] = (df['2ndFlrSF'] > 0).astype(int)\n",
        "        \n",
        "        # Quality mappings\n",
        "        quality_dict = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
        "        exterior_quality_dict = {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\n",
        "        \n",
        "        # Apply quality mappings\n",
        "        df['ExterQual'] = df['ExterQual'].replace(exterior_quality_dict)\n",
        "        df['ExterCond'] = df['ExterCond'].replace(exterior_quality_dict)\n",
        "        df['BsmtQual'] = df['BsmtQual'].replace(quality_dict)\n",
        "        df['BsmtCond'] = df['BsmtCond'].replace(quality_dict)\n",
        "        df['HeatingQC'] = df['HeatingQC'].replace(quality_dict)\n",
        "        df['KitchenQual'] = df['KitchenQual'].replace(quality_dict)\n",
        "        df['FireplaceQu'] = df['FireplaceQu'].replace(quality_dict)\n",
        "        df['GarageQual'] = df['GarageQual'].replace(quality_dict)\n",
        "        df['GarageCond'] = df['GarageCond'].replace(quality_dict)\n",
        "        df['PoolQC'] = df['PoolQC'].replace(quality_dict)\n",
        "        \n",
        "        # Basement exposure\n",
        "        bsmt_exposure_dict = {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\n",
        "        df['BsmtExposure'] = df['BsmtExposure'].replace(bsmt_exposure_dict)\n",
        "        \n",
        "        # Basement finish type\n",
        "        bsmt_finish_dict = {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\n",
        "        df['BsmtFinType1'] = df['BsmtFinType1'].replace(bsmt_finish_dict)\n",
        "        df['BsmtFinType2'] = df['BsmtFinType2'].replace(bsmt_finish_dict)\n",
        "        \n",
        "        # Functional mapping\n",
        "        functional_dict = {None: 0, 'Sal': 1, 'Sev': 2, 'Maj2': 3, 'Maj1': 4, \n",
        "                          'Mod': 5, 'Min2': 6, 'Min1': 7, 'Typ': 8}\n",
        "        df['Functional'] = df['Functional'].map(functional_dict).astype(int)\n",
        "        \n",
        "        # Fence mapping\n",
        "        fence_dict = {'None': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}\n",
        "        df['Fence'] = df['Fence'].map(fence_dict).astype(int)\n",
        "        \n",
        "        # Binary features\n",
        "        df['CentralAir'] = (df['CentralAir'] == 'Y').astype(int)\n",
        "        df['HasFireplace'] = (df['Fireplaces'] > 0).astype(int)\n",
        "        df['HasPool'] = (df['PoolArea'] > 0).astype(int)\n",
        "        df['HasWoodDeck'] = (df['WoodDeckSF'] > 0).astype(int)\n",
        "        df['HasEnclosedPorch'] = (df['EnclosedPorch'] > 0).astype(int)\n",
        "        df['Has3SsnPorch'] = (df['3SsnPorch'] > 0).astype(int)\n",
        "        df['HasScreenPorch'] = (df['ScreenPorch'] > 0).astype(int)\n",
        "        df['HasShed'] = (df['MiscFeature'] == 'Shed').astype(int)\n",
        "        \n",
        "        # Lot shape\n",
        "        df['LotShape'] = df['LotShape'].replace({'Reg': 1, 'IR1': 0, 'IR2': 0, 'IR3': 0})\n",
        "        \n",
        "        # Land slope\n",
        "        df['IsSlopeGentle'] = (df['LandSlope'] == 'Gtl').astype(int)\n",
        "        df = df.drop('LandSlope', axis=1)\n",
        "        \n",
        "        # MSSubClass encoding\n",
        "        df['IsNewerSubClass'] = df['MSSubClass'].replace({\n",
        "            20: 1, 30: 0, 40: 0, 45: 0, 50: 0, 60: 1, 70: 0, 75: 0, 80: 0, \n",
        "            85: 0, 90: 0, 120: 1, 150: 0, 160: 1, 180: 0, 190: 0\n",
        "        })\n",
        "        \n",
        "        # Railroad proximity\n",
        "        railroad_list = ['RRNn', 'RRAn', 'RRNe', 'RRAe']\n",
        "        df['RRProximity'] = df['Condition1'].apply(lambda x: 1 if x in railroad_list else 0)\n",
        "        \n",
        "        # Convert to string for dummification\n",
        "        df['MSSubClass'] = df['MSSubClass'].astype(str)\n",
        "        df['MoSold'] = df['MoSold'].astype(str)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _encode_categorical_variables(self, df):\n",
        "        \"\"\"Create dummy variables for categorical features.\"\"\"\n",
        "        # Get dummies for all categorical variables\n",
        "        df = pd.get_dummies(df, drop_first=True)\n",
        "        return df\n",
        "    \n",
        "    def _remove_unnecessary_columns(self, df):\n",
        "        \"\"\"Remove columns that don't add value or cause overfitting.\"\"\"\n",
        "        # Remove railroad condition dummies (already captured in RRProximity)\n",
        "        railroad_cols = [col for col in df.columns if 'Condition1_RR' in col]\n",
        "        df = df.drop(railroad_cols, axis=1)\n",
        "        \n",
        "        # Remove Id column\n",
        "        if 'Id' in df.columns:\n",
        "            df = df.drop('Id', axis=1)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def get_processed_data(self):\n",
        "        \"\"\"Return the processed data.\"\"\"\n",
        "        return self.processed_data\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = HousePricePreprocessor()\n",
        "print(\"Preprocessor class created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply preprocessing\n",
        "print(\"Starting data preprocessing...\")\n",
        "processed_data = preprocessor.preprocess(combined_data)\n",
        "\n",
        "# Split back into train and test sets\n",
        "train_size = len(train_data)\n",
        "train_processed = processed_data.iloc[:train_size].copy()\n",
        "test_processed = processed_data.iloc[train_size:].copy()\n",
        "\n",
        "# Extract target variable\n",
        "target = train_processed['SalePrice'].copy()\n",
        "train_processed = train_processed.drop('SalePrice', axis=1)\n",
        "test_processed = test_processed.drop('SalePrice', axis=1)\n",
        "\n",
        "print(f\"Processed training data shape: {train_processed.shape}\")\n",
        "print(f\"Processed test data shape: {test_processed.shape}\")\n",
        "print(f\"Number of features: {train_processed.shape[1]}\")\n",
        "print(f\"Missing values in processed data: {train_processed.isnull().sum().sum()}\")\n",
        "\n",
        "# Display feature information\n",
        "print(f\"\\nFeature types:\")\n",
        "print(f\"Numeric features: {train_processed.select_dtypes(include=[np.number]).shape[1]}\")\n",
        "print(f\"Categorical features: {train_processed.select_dtypes(include=['object']).shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Selection {#feature-selection}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureSelector:\n",
        "    \"\"\"\n",
        "    Feature selection class using multiple methods including RFECV and correlation analysis.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.selected_features = {}\n",
        "        self.feature_importance = {}\n",
        "        \n",
        "    def select_features_rfecv(self, X, y, models, cv=3):\n",
        "        \"\"\"\n",
        "        Select features using Recursive Feature Elimination with Cross-Validation.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pd.DataFrame\n",
        "            Training features\n",
        "        y : pd.Series\n",
        "            Target variable\n",
        "        models : dict\n",
        "            Dictionary of model names and instances\n",
        "        cv : int\n",
        "            Number of cross-validation folds\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        dict : Selected features for each model\n",
        "        \"\"\"\n",
        "        selected_features = {}\n",
        "        \n",
        "        for model_name, model in models.items():\n",
        "            print(f\"Selecting features for {model_name}...\")\n",
        "            \n",
        "            # Use RFECV\n",
        "            rfecv = RFECV(estimator=model, step=1, cv=cv, \n",
        "                         scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "            rfecv.fit(X, y)\n",
        "            \n",
        "            # Get selected features\n",
        "            selected_mask = rfecv.get_support()\n",
        "            selected_features[model_name] = X.columns[selected_mask].tolist()\n",
        "            \n",
        "            print(f\"  Selected {len(selected_features[model_name])} features\")\n",
        "            \n",
        "            # Store feature importance if available\n",
        "            if hasattr(rfecv.estimator_, 'feature_importances_'):\n",
        "                self.feature_importance[model_name] = dict(zip(\n",
        "                    selected_features[model_name], \n",
        "                    rfecv.estimator_.feature_importances_[:len(selected_features[model_name])]\n",
        "                ))\n",
        "        \n",
        "        self.selected_features = selected_features\n",
        "        return selected_features\n",
        "    \n",
        "    def get_correlation_features(self, X, y, threshold=0.1):\n",
        "        \"\"\"\n",
        "        Select features based on correlation with target variable.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pd.DataFrame\n",
        "            Training features\n",
        "        y : pd.Series\n",
        "            Target variable\n",
        "        threshold : float\n",
        "            Minimum absolute correlation threshold\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        list : Selected feature names\n",
        "        \"\"\"\n",
        "        # Calculate correlations\n",
        "        correlations = X.corrwith(y).abs()\n",
        "        \n",
        "        # Select features above threshold\n",
        "        selected = correlations[correlations > threshold].index.tolist()\n",
        "        \n",
        "        print(f\"Selected {len(selected)} features based on correlation threshold {threshold}\")\n",
        "        return selected\n",
        "\n",
        "# Initialize feature selector\n",
        "feature_selector = FeatureSelector()\n",
        "\n",
        "# Define models for feature selection\n",
        "models = {\n",
        "    'Ridge': Ridge(alpha=1.0),\n",
        "    'Lasso': Lasso(alpha=1.0),\n",
        "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Select features using RFECV\n",
        "print(\"Performing feature selection using RFECV...\")\n",
        "selected_features = feature_selector.select_features_rfecv(train_processed, target, models)\n",
        "\n",
        "# Display selected features for each model\n",
        "for model_name, features in selected_features.items():\n",
        "    print(f\"\\n{model_name} selected features ({len(features)}):\")\n",
        "    print(f\"  {features[:10]}{'...' if len(features) > 10 else ''}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training and Hyperparameter Tuning {#model-training}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelTrainer:\n",
        "    \"\"\"\n",
        "    Comprehensive model training and hyperparameter tuning class.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.trained_models = {}\n",
        "        self.best_params = {}\n",
        "        self.cv_scores = {}\n",
        "        \n",
        "    def train_models(self, X, y, selected_features, cv=3):\n",
        "        \"\"\"\n",
        "        Train and tune multiple models with their selected features.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pd.DataFrame\n",
        "            Training features\n",
        "        y : pd.Series\n",
        "            Target variable\n",
        "        selected_features : dict\n",
        "            Selected features for each model\n",
        "        cv : int\n",
        "            Number of cross-validation folds\n",
        "        \"\"\"\n",
        "        # Define parameter grids for each model\n",
        "        param_grids = {\n",
        "            'Ridge': {'alpha': np.logspace(-3, 3, 10)},\n",
        "            'Lasso': {'alpha': np.logspace(-3, 3, 10)},\n",
        "            'RandomForest': {\n",
        "                'n_estimators': [100, 200, 300],\n",
        "                'max_depth': [10, 20, None],\n",
        "                'min_samples_split': [2, 5, 10]\n",
        "            },\n",
        "            'GradientBoosting': {\n",
        "                'n_estimators': [100, 200, 300],\n",
        "                'learning_rate': [0.01, 0.1, 0.2],\n",
        "                'max_depth': [3, 5, 7]\n",
        "            },\n",
        "            'XGBoost': {\n",
        "                'n_estimators': [100, 200, 300],\n",
        "                'learning_rate': [0.01, 0.1, 0.2],\n",
        "                'max_depth': [3, 5, 7]\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Train each model\n",
        "        for model_name, features in selected_features.items():\n",
        "            print(f\"\\nTraining {model_name}...\")\n",
        "            \n",
        "            # Get features for this model\n",
        "            X_model = X[features]\n",
        "            \n",
        "            # Define base model\n",
        "            if model_name == 'Ridge':\n",
        "                base_model = Ridge()\n",
        "            elif model_name == 'Lasso':\n",
        "                base_model = Lasso()\n",
        "            elif model_name == 'RandomForest':\n",
        "                base_model = RandomForestRegressor(random_state=42)\n",
        "            elif model_name == 'GradientBoosting':\n",
        "                base_model = GradientBoostingRegressor(random_state=42)\n",
        "            elif model_name == 'XGBoost':\n",
        "                base_model = XGBRegressor(random_state=42)\n",
        "            \n",
        "            # Perform grid search\n",
        "            grid_search = GridSearchCV(\n",
        "                base_model, \n",
        "                param_grids[model_name], \n",
        "                cv=cv, \n",
        "                scoring='neg_mean_squared_error',\n",
        "                n_jobs=-1,\n",
        "                verbose=0\n",
        "            )\n",
        "            \n",
        "            # Fit the model\n",
        "            grid_search.fit(X_model, y)\n",
        "            \n",
        "            # Store results\n",
        "            self.trained_models[model_name] = grid_search.best_estimator_\n",
        "            self.best_params[model_name] = grid_search.best_params_\n",
        "            self.cv_scores[model_name] = -grid_search.best_score_\n",
        "            \n",
        "            print(f\"  Best parameters: {grid_search.best_params_}\")\n",
        "            print(f\"  CV RMSE: {np.sqrt(-grid_search.best_score_):.4f}\")\n",
        "    \n",
        "    def get_predictions(self, X, model_name):\n",
        "        \"\"\"\n",
        "        Get predictions from a trained model.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pd.DataFrame\n",
        "            Features for prediction\n",
        "        model_name : str\n",
        "            Name of the model\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        np.array : Predictions\n",
        "        \"\"\"\n",
        "        if model_name not in self.trained_models:\n",
        "            raise ValueError(f\"Model {model_name} not found\")\n",
        "        \n",
        "        # Get selected features for this model\n",
        "        features = selected_features[model_name]\n",
        "        X_model = X[features]\n",
        "        \n",
        "        # Make predictions\n",
        "        return self.trained_models[model_name].predict(X_model)\n",
        "    \n",
        "    def get_model_performance(self):\n",
        "        \"\"\"\n",
        "        Get performance summary of all trained models.\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame : Performance metrics\n",
        "        \"\"\"\n",
        "        performance_data = []\n",
        "        \n",
        "        for model_name in self.trained_models.keys():\n",
        "            performance_data.append({\n",
        "                'Model': model_name,\n",
        "                'CV_RMSE': self.cv_scores[model_name],\n",
        "                'Features_Used': len(selected_features[model_name])\n",
        "            })\n",
        "        \n",
        "        return pd.DataFrame(performance_data).sort_values('CV_RMSE')\n",
        "\n",
        "# Initialize model trainer\n",
        "model_trainer = ModelTrainer()\n",
        "\n",
        "# Train all models\n",
        "print(\"Starting model training and hyperparameter tuning...\")\n",
        "model_trainer.train_models(train_processed, target, selected_features)\n",
        "\n",
        "# Display performance summary\n",
        "print(\"\\nModel Performance Summary:\")\n",
        "performance_df = model_trainer.get_model_performance()\n",
        "print(performance_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Stacking {#model-stacking}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelStacker:\n",
        "    \"\"\"\n",
        "    Model stacking class for creating ensemble predictions.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.stacked_model = None\n",
        "        self.level0_predictions = None\n",
        "        \n",
        "    def create_level0_predictions(self, X, y, model_trainer, selected_features):\n",
        "        \"\"\"\n",
        "        Create Level-0 predictions from all base models.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pd.DataFrame\n",
        "            Training features\n",
        "        y : pd.Series\n",
        "            Target variable\n",
        "        model_trainer : ModelTrainer\n",
        "            Trained model trainer instance\n",
        "        selected_features : dict\n",
        "            Selected features for each model\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame : Level-0 predictions\n",
        "        \"\"\"\n",
        "        # Split data for stacking\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        \n",
        "        # Create Level-0 predictions\n",
        "        level0_predictions = {}\n",
        "        \n",
        "        for model_name in model_trainer.trained_models.keys():\n",
        "            print(f\"Creating Level-0 predictions for {model_name}...\")\n",
        "            \n",
        "            # Get features for this model\n",
        "            features = selected_features[model_name]\n",
        "            X_model_train = X_train[features]\n",
        "            X_model_val = X_val[features]\n",
        "            \n",
        "            # Train on training set\n",
        "            model = model_trainer.trained_models[model_name]\n",
        "            model.fit(X_model_train, y_train)\n",
        "            \n",
        "            # Predict on validation set\n",
        "            val_predictions = model.predict(X_model_val)\n",
        "            level0_predictions[model_name] = val_predictions\n",
        "        \n",
        "        # Create DataFrame of Level-0 predictions\n",
        "        self.level0_predictions = pd.DataFrame(level0_predictions, index=X_val.index)\n",
        "        \n",
        "        return self.level0_predictions, y_val\n",
        "    \n",
        "    def train_stacked_model(self, X, y, meta_models=['LinearRegression', 'GradientBoosting', 'XGBoost']):\n",
        "        \"\"\"\n",
        "        Train the Level-1 (stacked) model.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pd.DataFrame\n",
        "            Level-0 predictions\n",
        "        y : pd.Series\n",
        "            Target variable\n",
        "        meta_models : list\n",
        "            List of meta-models to try\n",
        "        \"\"\"\n",
        "        best_score = float('inf')\n",
        "        best_model = None\n",
        "        best_name = None\n",
        "        \n",
        "        for model_name in meta_models:\n",
        "            print(f\"Training stacked model: {model_name}\")\n",
        "            \n",
        "            if model_name == 'LinearRegression':\n",
        "                model = LinearRegression()\n",
        "            elif model_name == 'GradientBoosting':\n",
        "                model = GradientBoostingRegressor(random_state=42)\n",
        "            elif model_name == 'XGBoost':\n",
        "                model = XGBRegressor(random_state=42)\n",
        "            \n",
        "            # Cross-validation score\n",
        "            cv_scores = cross_val_score(model, X, y, cv=3, scoring='neg_mean_squared_error')\n",
        "            cv_rmse = np.sqrt(-cv_scores.mean())\n",
        "            \n",
        "            print(f\"  CV RMSE: {cv_rmse:.4f}\")\n",
        "            \n",
        "            if cv_rmse < best_score:\n",
        "                best_score = cv_rmse\n",
        "                best_model = model\n",
        "                best_name = model_name\n",
        "        \n",
        "        # Train best model on full data\n",
        "        best_model.fit(X, y)\n",
        "        self.stacked_model = best_model\n",
        "        \n",
        "        print(f\"\\nBest stacked model: {best_name} with RMSE: {best_score:.4f}\")\n",
        "    \n",
        "    def predict_stacked(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the stacked model.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pd.DataFrame\n",
        "            Level-0 predictions\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        np.array : Stacked predictions\n",
        "        \"\"\"\n",
        "        if self.stacked_model is None:\n",
        "            raise ValueError(\"Stacked model not trained yet\")\n",
        "        \n",
        "        return self.stacked_model.predict(X)\n",
        "\n",
        "# Initialize model stacker\n",
        "stacker = ModelStacker()\n",
        "\n",
        "# Create Level-0 predictions\n",
        "print(\"Creating Level-0 predictions...\")\n",
        "level0_preds, y_val = stacker.create_level0_predictions(train_processed, target, model_trainer, selected_features)\n",
        "\n",
        "# Train stacked model\n",
        "print(\"\\nTraining stacked model...\")\n",
        "stacker.train_stacked_model(level0_preds, y_val)\n",
        "\n",
        "# Evaluate stacked model\n",
        "stacked_score = np.sqrt(mean_squared_error(y_val, stacker.predict_stacked(level0_preds)))\n",
        "print(f\"Stacked model validation RMSE: {stacked_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Final Predictions and Results {#results}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create final predictions for test set\n",
        "print(\"Creating final predictions for test set...\")\n",
        "\n",
        "# Get Level-0 predictions for test set\n",
        "test_level0_predictions = {}\n",
        "\n",
        "for model_name in model_trainer.trained_models.keys():\n",
        "    print(f\"Generating test predictions for {model_name}...\")\n",
        "    \n",
        "    # Get features for this model\n",
        "    features = selected_features[model_name]\n",
        "    X_test_model = test_processed[features]\n",
        "    \n",
        "    # Make predictions\n",
        "    test_predictions = model_trainer.get_predictions(test_processed, model_name)\n",
        "    test_level0_predictions[model_name] = test_predictions\n",
        "\n",
        "# Create DataFrame of test Level-0 predictions\n",
        "test_level0_df = pd.DataFrame(test_level0_predictions)\n",
        "\n",
        "# Generate final stacked predictions\n",
        "final_predictions = stacker.predict_stacked(test_level0_df)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'Id': range(1461, 1461 + len(final_predictions)),\n",
        "    'SalePrice': final_predictions\n",
        "})\n",
        "\n",
        "# Save submission\n",
        "submission.to_csv('final_submission.csv', index=False)\n",
        "print(f\"Final submission saved with {len(submission)} predictions\")\n",
        "\n",
        "# Display prediction statistics\n",
        "print(f\"\\nPrediction Statistics:\")\n",
        "print(f\"Mean prediction: ${final_predictions.mean():,.2f}\")\n",
        "print(f\"Min prediction: ${final_predictions.min():,.2f}\")\n",
        "print(f\"Max prediction: ${final_predictions.max():,.2f}\")\n",
        "print(f\"Std prediction: ${final_predictions.std():,.2f}\")\n",
        "\n",
        "# Display first few predictions\n",
        "print(f\"\\nFirst 10 predictions:\")\n",
        "print(submission.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Performance Visualization\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# 1. Individual Model Performance\n",
        "plt.subplot(2, 3, 1)\n",
        "performance_df = model_trainer.get_model_performance()\n",
        "plt.bar(performance_df['Model'], performance_df['CV_RMSE'])\n",
        "plt.title('Individual Model Performance (CV RMSE)')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# 2. Feature Importance (Random Forest)\n",
        "plt.subplot(2, 3, 2)\n",
        "if 'RandomForest' in feature_selector.feature_importance:\n",
        "    rf_importance = feature_selector.feature_importance['RandomForest']\n",
        "    top_features = sorted(rf_importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    features, importance = zip(*top_features)\n",
        "    plt.barh(features, importance)\n",
        "    plt.title('Top 10 Features (Random Forest)')\n",
        "    plt.xlabel('Importance')\n",
        "\n",
        "# 3. Prediction Distribution\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.hist(final_predictions, bins=50, alpha=0.7, color='skyblue')\n",
        "plt.title('Final Prediction Distribution')\n",
        "plt.xlabel('Predicted Sale Price ($)')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# 4. Level-0 Predictions Correlation\n",
        "plt.subplot(2, 3, 4)\n",
        "correlation_matrix = test_level0_df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Level-0 Predictions Correlation')\n",
        "\n",
        "# 5. Model Comparison\n",
        "plt.subplot(2, 3, 5)\n",
        "model_names = list(selected_features.keys())\n",
        "feature_counts = [len(features) for features in selected_features.values()]\n",
        "plt.bar(model_names, feature_counts)\n",
        "plt.title('Number of Features per Model')\n",
        "plt.ylabel('Feature Count')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# 6. Prediction vs Actual (for validation set)\n",
        "plt.subplot(2, 3, 6)\n",
        "if hasattr(stacker, 'level0_predictions') and stacker.level0_predictions is not None:\n",
        "    stacked_val_preds = stacker.predict_stacked(stacker.level0_predictions)\n",
        "    plt.scatter(y_val, stacked_val_preds, alpha=0.6)\n",
        "    plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
        "    plt.xlabel('Actual Sale Price')\n",
        "    plt.ylabel('Predicted Sale Price')\n",
        "    plt.title('Prediction vs Actual (Validation)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final Summary\n",
        "print(\"=\"*60)\n",
        "print(\"FINAL PROJECT SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Dataset: Ames Housing (2,919 properties, 80+ features)\")\n",
        "print(f\"Preprocessing: Outlier removal, feature engineering, encoding\")\n",
        "print(f\"Feature Selection: RFECV for each model\")\n",
        "print(f\"Models Trained: {len(model_trainer.trained_models)}\")\n",
        "print(f\"Best Individual Model: {performance_df.iloc[0]['Model']} (RMSE: {performance_df.iloc[0]['CV_RMSE']:.4f})\")\n",
        "print(f\"Stacked Model: {type(stacker.stacked_model).__name__}\")\n",
        "print(f\"Final Predictions: {len(final_predictions)} test samples\")\n",
        "print(f\"Submission File: final_submission.csv\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
